---
title: A Study on Trust Region Update Rules in Newton Methods for Large-scale Linear
  Classification
abstract: The main task in training a linear classifier is to solve an unconstrained
  minimization problem. To apply an optimization method typically we iteratively find
  a good direction and then decide a suitable step size. Past developments of extending
  optimization methods for large-scale linear classification focus on finding the
  direction, but little attention has been paid on adjusting the step size. In this
  work, we explain that inappropriate step-size adjustment may lead to serious slow
  convergence. Among the two major methods for step-size selection, line search and
  trust region, we focus on investigating the trust region methods. After presenting
  some detailed analysis, we develop novel and effective techniques to adjust the
  trust-region size. Experiments indicate that our new settings significantly outperform
  existing implementations for large-scale linear classification.
section: Accepted Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hsia17a
month: 0
tex_title: A Study on Trust Region Update Rules in Newton Methods for Large-scale
  Linear Classification
firstpage: 33
lastpage: 48
page: 33-48
order: 33
cycles: false
author:
- given: Chih-Yang
  family: Hsia
- given: Ya
  family: Zhu
- given: Chih-Jen
  family: Lin
date: 2017-11-11
address: 
publisher: PMLR
container-title: Proceedings of the Ninth Asian Conference on Machine Learning
volume: '77'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 11
  - 11
pdf: http://proceedings.mlr.press/v77/hsia17a/hsia17a.pdf
extras:
- label: Supplementary TAR
  link: http://proceedings.mlr.press/v77/hsia17a/hsia17a-supp.tar
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
