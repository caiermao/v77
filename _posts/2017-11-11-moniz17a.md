---
title: Nested LSTMs
abstract: We propose \emphNested LSTMs (NLSTM), a novel RNN architecture with multiple
  levels of memory. Nested LSTMs add depth to LSTMs via nesting as opposed to stacking.
  The value of a memory cell in an NLSTM is computed by an LSTM cell, which has its
  own \it inner memory cell. Specifically, instead of computing the value of the (outer)
  memory cell as $c^outer_t = f_t ⊙c_t-1 + i_t ⊙g_t$, NLSTM memory cells use the concatenation
  $(f_t ⊙c_t-1, i_t ⊙g_t)$ as input to an inner LSTM (or NLSTM) memory cell, and set
  $c^outer_t$ = $h^inner_t$. Nested LSTMs outperform both stacked and single-layer
  LSTMs with similar numbers of parameters in our experiments on various character-level
  language modeling tasks, and the inner memories of an LSTM learn longer term dependencies
  compared with the higher-level units of a stacked LSTM.
section: Accepted Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: moniz17a
month: 0
tex_title: Nested LSTMs
firstpage: 530
lastpage: 544
page: 530-544
order: 530
cycles: false
author:
- given: Joel Ruben Antony
  family: Moniz
- given: David
  family: Krueger
date: 2017-11-11
address: 
publisher: PMLR
container-title: Proceedings of the Ninth Asian Conference on Machine Learning
volume: '77'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 11
  - 11
pdf: http://proceedings.mlr.press/v77/moniz17a/moniz17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
