---
title: Scale-Invariant Recognition by Weight-Shared CNNs in Parallel
abstract: Deep convolutional neural networks (CNNs) have become one of the most successful
  methods for image processing tasks in past few years. Recent studies on modern residual
  architectures, enabling CNNs to be much deeper, have achieved much better results
  thanks to their high expressive ability by numerous parameters. In general, CNNs
  are known to have the robustness to the small parallel shift of objects in images
  by their local receptive fields, weight parameters shared by each unit, and pooling
  layers sandwiching them. However, CNNs have a limited robustness to the other geometric
  transformations such as scaling and rotation, and this lack becomes an obstacle
  to performance improvement even now. This paper proposes a novel network architecture,
  the \emphweight-shared multi-stage network (WSMS-Net), and focuses on acquiring
  the scale invariance by constructing of multiple stages of CNNs. The WSMS-Net is
  easily combined with existing deep CNNs, enables existing deep CNNs to acquire a
  robustness to the scaling, and therefore, achieves higher classification accuracy
  on CIFAR-10, CIFAR-100 and ImageNet datasets.
section: Accepted Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: takahashi17a
month: 0
tex_title: Scale-Invariant Recognition by Weight-Shared CNNs in Parallel
firstpage: 295
lastpage: 310
page: 295-310
order: 295
cycles: false
author:
- given: Ryo
  family: Takahashi
- given: Takashi
  family: Matsubara
- given: Kuniaki
  family: Uehara
date: 2017-11-11
address: 
publisher: PMLR
container-title: Proceedings of the Ninth Asian Conference on Machine Learning
volume: '77'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 11
  - 11
pdf: http://proceedings.mlr.press/v77/takahashi17a/takahashi17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
