---
title: On the Flatness of Loss Surface for Two-layered ReLU Networks
abstract: Deep learning has achieved unprecedented practical success in many applications.
  Despite its empirical success, however, the theoretical understanding of deep neural
  networks still remains a major open problem. In this paper, we explore properties
  of two-layered ReLU networks. For simplicity, we assume that the optimal model parameters
  (also called ground-truth parameters) are known. We then assume that a network receives
  Gaussian input and is trained by minimizing the expected squared loss between the
  prediction function of the network and a target function. To conduct the analysis,
  we propose a normal equation for critical points, and study the invariances under
  three kinds of transformations, namely, scale transformation, rotation transformation
  and perturbation transformation. We prove that these transformations can keep the
  loss of a critical point invariant, thus can incur flat regions. Consequently, how
  to escape from flat regions is vital in training neural networks.
section: Accepted Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cao17a
month: 0
tex_title: On the Flatness of Loss Surface for Two-layered ReLU Networks
firstpage: 545
lastpage: 560
page: 545-560
order: 545
cycles: false
author:
- given: Jiezhang
  family: Cao
- given: Qingyao
  family: Wu
- given: Yuguang
  family: Yan
- given: Li
  family: Wang
- given: Mingkui
  family: Tan
date: 2017-11-11
address: 
publisher: PMLR
container-title: Proceedings of the Ninth Asian Conference on Machine Learning
volume: '77'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 11
  - 11
pdf: http://proceedings.mlr.press/v77/cao17a/cao17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
