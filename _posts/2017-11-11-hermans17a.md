---
title: Accumulated Gradient Normalization
abstract: This work addresses the instability in asynchronous data parallel optimization.
  It does so by introducing a novel distributed optimizer which is able to efficiently
  optimize a centralized model under communication constraints. The optimizer achieves
  this by pushing a normalized sequence of first-order gradients to a parameter server.
  This implies that the magnitude of a worker delta is smaller compared to an accumulated
  gradient, and provides a better direction towards a minimum compared to first-order
  gradients, which in turn also forces possible implicit momentum fluctuations to
  be more aligned since we make the assumption that all workers contribute towards
  a single minima. As a result, our approach mitigates the parameter staleness problem
  more effectively since staleness in asynchrony induces (implicit) momentum, and
  achieves a better convergence rate compared to other optimizers such as asynchronous
  \textsceasgd and \textscdynsgd, which we show empirically.
section: Accepted Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hermans17a
month: 0
tex_title: Accumulated Gradient Normalization
firstpage: 439
lastpage: 454
page: 439-454
order: 439
cycles: false
author:
- given: Joeri R.
  family: Hermans
- given: Gerasimos
  family: Spanakis
- given: Rico
  family: MÃ¶ckel
date: 2017-11-11
address: 
publisher: PMLR
container-title: Proceedings of the Ninth Asian Conference on Machine Learning
volume: '77'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 11
  - 11
pdf: http://proceedings.mlr.press/v77/hermans17a/hermans17a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v77/hermans17a/hermans17a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
